{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-15T11:01:11.224688Z",
     "iopub.status.busy": "2025-11-15T11:01:11.223907Z",
     "iopub.status.idle": "2025-11-15T11:01:11.235223Z",
     "shell.execute_reply": "2025-11-15T11:01:11.234236Z",
     "shell.execute_reply.started": "2025-11-15T11:01:11.224655Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "class LoyaltyPointsEDA:\n",
    "    def __init__(self, df):\n",
    "        \"\"\"Initialize with transaction dataframe\"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.cleaned_df = None\n",
    "        self.customer_summary = None\n",
    "        \n",
    "    def initial_exploration(self):\n",
    "        \"\"\"Perform initial data exploration\"\"\"\n",
    "        print(\"=\"*80)\n",
    "        print(\"INITIAL DATA EXPLORATION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\nDataset Shape: {self.df.shape}\")\n",
    "        print(f\"\\nColumns: {list(self.df.columns)}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"Data Types:\")\n",
    "        print(\"-\"*80)\n",
    "        print(self.df.dtypes)\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"First Few Rows:\")\n",
    "        print(\"-\"*80)\n",
    "        print(self.df.head())\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"Statistical Summary:\")\n",
    "        print(\"-\"*80)\n",
    "        print(self.df.describe())\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"Missing Values:\")\n",
    "        print(\"-\"*80)\n",
    "        missing = self.df.isnull().sum()\n",
    "        missing_pct = (missing / len(self.df)) * 100\n",
    "        missing_df = pd.DataFrame({\n",
    "            'Missing_Count': missing,\n",
    "            'Percentage': missing_pct\n",
    "        })\n",
    "        print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"Duplicate Rows:\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"Number of duplicates: {self.df.duplicated().sum()}\")\n",
    "        \n",
    "    def clean_data(self):\n",
    "        \"\"\"Clean and preprocess the data\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"DATA CLEANING\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        df_clean = self.df.copy()\n",
    "        \n",
    "        print(\"\\n1. Handling Missing Values...\")\n",
    "        for col in df_clean.columns:\n",
    "            if df_clean[col].isnull().sum() > 0:\n",
    "                if df_clean[col].dtype in ['float64', 'int64']:\n",
    "                    df_clean[col].fillna(df_clean[col].median(), inplace=True)\n",
    "                    print(f\"   - Filled {col} with median\")\n",
    "                else:\n",
    "                    df_clean[col].fillna('Unknown', inplace=True)\n",
    "                    print(f\"   - Filled {col} with 'Unknown'\")\n",
    "        \n",
    "        print(\"\\n2. Removing Duplicates...\")\n",
    "        before = len(df_clean)\n",
    "        df_clean.drop_duplicates(inplace=True)\n",
    "        after = len(df_clean)\n",
    "        print(f\"   - Removed {before - after} duplicate rows\")\n",
    "        \n",
    "        print(\"\\n3. Converting Date Columns...\")\n",
    "        date_cols = [col for col in df_clean.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "        for col in date_cols:\n",
    "            try:\n",
    "                df_clean[col] = pd.to_datetime(df_clean[col])\n",
    "                print(f\"   - Converted {col} to datetime\")\n",
    "            except:\n",
    "                print(f\"   - Could not convert {col}\")\n",
    "        \n",
    "        print(\"\\n4. Detecting Outliers (IQR method)...\")\n",
    "        numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            Q1 = df_clean[col].quantile(0.25)\n",
    "            Q3 = df_clean[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = ((df_clean[col] < (Q1 - 1.5 * IQR)) | (df_clean[col] > (Q3 + 1.5 * IQR))).sum()\n",
    "            if outliers > 0:\n",
    "                print(f\"   - {col}: {outliers} outliers detected ({outliers/len(df_clean)*100:.2f}%)\")\n",
    "        \n",
    "        self.cleaned_df = df_clean\n",
    "        print(\"\\nData cleaning completed!\")\n",
    "        return df_clean\n",
    "    \n",
    "    def visualize_data(self):\n",
    "        \"\"\"Create visualizations for EDA\"\"\"\n",
    "        if self.cleaned_df is None:\n",
    "            print(\"Please run clean_data() first!\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"DATA VISUALIZATIONS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        df = self.cleaned_df\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        # Distribution plots\n",
    "        if len(numeric_cols) > 0:\n",
    "            fig, axes = plt.subplots(min(len(numeric_cols), 3), 1, figsize=(12, 4*min(len(numeric_cols), 3)))\n",
    "            if len(numeric_cols) == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            for i, col in enumerate(numeric_cols[:3]):\n",
    "                df[col].hist(bins=30, ax=axes[i], edgecolor='black')\n",
    "                axes[i].set_title(f'Distribution of {col}')\n",
    "                axes[i].set_xlabel(col)\n",
    "                axes[i].set_ylabel('Frequency')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        if len(numeric_cols) > 1:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "            plt.title('Correlation Heatmap')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    def calculate_loyalty_points(self, transaction_amt_col, points_rate=0.1):\n",
    "        \"\"\"\n",
    "        Calculate accrued points for current transactions\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        transaction_amt_col : str\n",
    "            Column name containing transaction amount\n",
    "        points_rate : float\n",
    "            Points earned per dollar (default: 0.1 = 10 points per $100)\n",
    "        \"\"\"\n",
    "        if self.cleaned_df is None:\n",
    "            print(\"Please run clean_data() first!\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"CALCULATING LOYALTY POINTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        df = self.cleaned_df.copy()\n",
    "        \n",
    "        df['accrued_points'] = df[transaction_amt_col] * points_rate\n",
    "        df['accrued_points'] = df['accrued_points'].round(2)\n",
    "        \n",
    "        print(f\"\\nPoints Rate: {points_rate} points per dollar\")\n",
    "        print(f\"Total Points Accrued: {df['accrued_points'].sum():,.2f}\")\n",
    "        print(f\"Average Points per Transaction: {df['accrued_points'].mean():,.2f}\")\n",
    "        \n",
    "        self.cleaned_df = df\n",
    "        return df\n",
    "    \n",
    "    def update_customer_balance(self, customer_id_col, initial_balance=0):\n",
    "        \"\"\"\n",
    "        Update customer's total point balance\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        customer_id_col : str\n",
    "            Column name containing customer ID\n",
    "        initial_balance : float\n",
    "            Initial balance for new customers\n",
    "        \"\"\"\n",
    "        if self.cleaned_df is None or 'accrued_points' not in self.cleaned_df.columns:\n",
    "            print(\"Please run calculate_loyalty_points() first!\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"UPDATING CUSTOMER POINT BALANCES\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        df = self.cleaned_df.copy()\n",
    "        \n",
    "        customer_points = df.groupby(customer_id_col).agg({\n",
    "            'accrued_points': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        customer_points.rename(columns={'accrued_points': 'total_point_balance'}, inplace=True)\n",
    "        customer_points['total_point_balance'] += initial_balance\n",
    "        \n",
    "        df = df.merge(customer_points, on=customer_id_col, how='left')\n",
    "        \n",
    "        print(f\"\\nTotal Customers: {df[customer_id_col].nunique()}\")\n",
    "        print(f\"Total Points Distributed: {customer_points['total_point_balance'].sum():,.2f}\")\n",
    "        print(f\"Average Balance per Customer: {customer_points['total_point_balance'].mean():,.2f}\")\n",
    "        print(f\"Top Customer Balance: {customer_points['total_point_balance'].max():,.2f}\")\n",
    "        \n",
    "        self.cleaned_df = df\n",
    "        self.customer_summary = customer_points\n",
    "        return df, customer_points\n",
    "    \n",
    "    def calculate_rfm(self, customer_id_col, date_col, transaction_amt_col, reference_date=None):\n",
    "        \"\"\"\n",
    "        Calculate RFM (Recency, Frequency, Monetary) metrics\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        customer_id_col : str\n",
    "            Column name containing customer ID\n",
    "        date_col : str\n",
    "            Column name containing transaction date\n",
    "        transaction_amt_col : str\n",
    "            Column name containing transaction amount\n",
    "        reference_date : datetime\n",
    "            Reference date for recency calculation (default: max date in data)\n",
    "        \"\"\"\n",
    "        if self.cleaned_df is None:\n",
    "            print(\"Please run clean_data() first!\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"CALCULATING RFM METRICS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        df = self.cleaned_df.copy()\n",
    "        \n",
    "        if reference_date is None:\n",
    "            reference_date = df[date_col].max()\n",
    "        \n",
    "        rfm = df.groupby(customer_id_col).agg({\n",
    "            date_col: lambda x: (reference_date - x.max()).days,  # Recency\n",
    "            customer_id_col: 'count',\n",
    "            transaction_amt_col: 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        rfm.columns = [customer_id_col, 'recency', 'frequency', 'monetary']\n",
    "        \n",
    "        rfm['R_score'] = pd.qcut(rfm['recency'], 5, labels=[5,4,3,2,1], duplicates='drop')\n",
    "        rfm['F_score'] = pd.qcut(rfm['frequency'].rank(method='first'), 5, labels=[1,2,3,4,5], duplicates='drop')\n",
    "        rfm['M_score'] = pd.qcut(rfm['monetary'], 5, labels=[1,2,3,4,5], duplicates='drop')\n",
    "        \n",
    "        rfm['R_score'] = rfm['R_score'].astype(int)\n",
    "        rfm['F_score'] = rfm['F_score'].astype(int)\n",
    "        rfm['M_score'] = rfm['M_score'].astype(int)\n",
    "        \n",
    "        rfm['RFM_score'] = rfm['R_score'] + rfm['F_score'] + rfm['M_score']\n",
    "        \n",
    "        rfm['customer_segment'] = rfm['RFM_score'].apply(self._segment_customer)\n",
    "        \n",
    "        self.cleaned_df = df.merge(rfm[[customer_id_col, 'recency', 'frequency', 'monetary', \n",
    "                                         'R_score', 'F_score', 'M_score', 'RFM_score', \n",
    "                                         'customer_segment']], on=customer_id_col, how='left')\n",
    "        \n",
    "        print(f\"\\nRFM Analysis Summary:\")\n",
    "        print(f\"Reference Date: {reference_date.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"\\nRecency (days since last purchase):\")\n",
    "        print(f\"  - Mean: {rfm['recency'].mean():.1f} days\")\n",
    "        print(f\"  - Median: {rfm['recency'].median():.1f} days\")\n",
    "        print(f\"\\nFrequency (number of purchases):\")\n",
    "        print(f\"  - Mean: {rfm['frequency'].mean():.1f} transactions\")\n",
    "        print(f\"  - Median: {rfm['frequency'].median():.1f} transactions\")\n",
    "        print(f\"\\nMonetary (total spend):\")\n",
    "        print(f\"  - Mean: ${rfm['monetary'].mean():,.2f}\")\n",
    "        print(f\"  - Median: ${rfm['monetary'].median():,.2f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"Customer Segmentation:\")\n",
    "        print(\"-\"*80)\n",
    "        print(rfm['customer_segment'].value_counts().sort_index())\n",
    "        \n",
    "        return self.cleaned_df, rfm\n",
    "    \n",
    "    def _segment_customer(self, score):\n",
    "        \"\"\"Segment customers based on RFM score\"\"\"\n",
    "        if score >= 13:\n",
    "            return 'Champions'\n",
    "        elif score >= 10:\n",
    "            return 'Loyal Customers'\n",
    "        elif score >= 7:\n",
    "            return 'Potential Loyalists'\n",
    "        elif score >= 5:\n",
    "            return 'At Risk'\n",
    "        else:\n",
    "            return 'Lost'\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate comprehensive summary report\"\"\"\n",
    "        if self.cleaned_df is None:\n",
    "            print(\"Please complete the analysis pipeline first!\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"COMPREHENSIVE LOYALTY PROGRAM REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        df = self.cleaned_df\n",
    "        \n",
    "        print(\"\\nDATASET OVERVIEW\")\n",
    "        print(f\"  - Total Transactions: {len(df):,}\")\n",
    "        print(f\"  - Date Range: {df[df.select_dtypes(include=['datetime64']).columns[0]].min().strftime('%Y-%m-%d')} to {df[df.select_dtypes(include=['datetime64']).columns[0]].max().strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        if 'accrued_points' in df.columns:\n",
    "            print(\"\\nPOINTS SUMMARY\")\n",
    "            print(f\"  - Total Points Accrued: {df['accrued_points'].sum():,.2f}\")\n",
    "            print(f\"  - Average Points per Transaction: {df['accrued_points'].mean():,.2f}\")\n",
    "        \n",
    "        if 'total_point_balance' in df.columns:\n",
    "            print(\"\\nCUSTOMER BALANCES\")\n",
    "            print(f\"  - Total Customers: {df[df.columns[0]].nunique():,}\")\n",
    "            print(f\"  - Average Balance: {df.groupby(df.columns[0])['total_point_balance'].first().mean():,.2f}\")\n",
    "        \n",
    "        if 'customer_segment' in df.columns:\n",
    "            print(\"\\nCUSTOMER SEGMENTS\")\n",
    "            segments = df.groupby(df.columns[0])['customer_segment'].first().value_counts()\n",
    "            for segment, count in segments.items():\n",
    "                print(f\"  - {segment}: {count} ({count/len(segments)*100:.1f}%)\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "\n",
    "def run_loyalty_analysis(file_path, \n",
    "                         customer_id_col='customer_id',\n",
    "                         date_col='transaction_date', \n",
    "                         transaction_amt_col='amount',\n",
    "                         points_rate=0.1,\n",
    "                         initial_balance=0,\n",
    "                         output_folder='output'):\n",
    "    \"\"\"\n",
    "    Complete loyalty points analysis pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to CSV file containing transaction data\n",
    "    customer_id_col : str\n",
    "        Column name for customer ID\n",
    "    date_col : str\n",
    "        Column name for transaction date\n",
    "    transaction_amt_col : str\n",
    "        Column name for transaction amount\n",
    "    points_rate : float\n",
    "        Points earned per dollar (default: 0.1)\n",
    "    initial_balance : float\n",
    "        Initial balance for customers\n",
    "    output_folder : str\n",
    "        Folder to save output files\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df_final : DataFrame\n",
    "        Cleaned data with all calculations\n",
    "    rfm_df : DataFrame\n",
    "        RFM analysis summary\n",
    "    customer_summary : DataFrame\n",
    "        Customer point balances\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    print(\"LOYALTY POINTS ANALYSIS PIPELINE\")\n",
    "    \n",
    "    print(\"\\nLoading data from:\", file_path)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Successfully loaded {len(df):,} rows and {len(df.columns)} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    print(\"\\nðŸ”§ Initializing analysis...\")\n",
    "    eda = LoyaltyPointsEDA(df)\n",
    "    \n",
    "    eda.initial_exploration()\n",
    "    \n",
    "    df_clean = eda.clean_data()\n",
    "    \n",
    "    try:\n",
    "        eda.visualize_data()\n",
    "    except:\n",
    "        print(\"Skipping visualizations\")\n",
    "    \n",
    "    eda.calculate_loyalty_points(\n",
    "        transaction_amt_col=transaction_amt_col, \n",
    "        points_rate=points_rate\n",
    "    )\n",
    "    \n",
    "    df_with_balance, customer_summary = eda.update_customer_balance(\n",
    "        customer_id_col=customer_id_col,\n",
    "        initial_balance=initial_balance\n",
    "    )\n",
    "    \n",
    "    df_final, rfm_df = eda.calculate_rfm(\n",
    "        customer_id_col=customer_id_col,\n",
    "        date_col=date_col,\n",
    "        transaction_amt_col=transaction_amt_col\n",
    "    )\n",
    "    \n",
    "    eda.generate_report()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPORTING RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    output_files = {\n",
    "        'cleaned_data': os.path.join(output_folder, 'cleaned_loyalty_data.csv'),\n",
    "        'rfm_analysis': os.path.join(output_folder, 'rfm_analysis.csv'),\n",
    "        'customer_summary': os.path.join(output_folder, 'customer_point_balances.csv')\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        df_final.to_csv(output_files['cleaned_data'], index=False)\n",
    "        print(f\"Cleaned data saved to: {output_files['cleaned_data']}\")\n",
    "        \n",
    "        rfm_df.to_csv(output_files['rfm_analysis'], index=False)\n",
    "        print(f\"RFM analysis saved to: {output_files['rfm_analysis']}\")\n",
    "        \n",
    "        customer_summary.to_csv(output_files['customer_summary'], index=False)\n",
    "        print(f\"Customer balances saved to: {output_files['customer_summary']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving files: {e}\")\n",
    "    \n",
    "    print(\"ANALYSIS COMPLETE!\")\n",
    "    \n",
    "    return df_final, rfm_df, customer_summary\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_final, rfm_df, customer_summary = run_loyalty_analysis(\n",
    "        file_path='transactions.csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T10:12:11.975744Z",
     "iopub.status.busy": "2025-11-15T10:12:11.974814Z",
     "iopub.status.idle": "2025-11-15T10:12:27.831794Z",
     "shell.execute_reply": "2025-11-15T10:12:27.830778Z",
     "shell.execute_reply.started": "2025-11-15T10:12:11.975712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "\n",
    "def perform_eda(file_path):\n",
    "    \"\"\"\n",
    "    Perform comprehensive Exploratory Data Analysis on a dataset\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df : DataFrame\n",
    "        Loaded dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"EXPLORATORY DATA ANALYSIS (EDA)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nðŸ“ STEP 1: LOADING DATA\")\n",
    "    print(\"-\"*80)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"âœ… Successfully loaded: {file_path}\")\n",
    "        print(f\"   Rows: {df.shape[0]:,}\")\n",
    "        print(f\"   Columns: {df.shape[1]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading file: {e}\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“Š STEP 2: BASIC INFORMATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nðŸ”¹ Column Names:\")\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        print(f\"   {i}. {col}\")\n",
    "    \n",
    "    print(\"\\nðŸ”¹ Data Types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"\\nðŸ”¹ Dataset Shape:\")\n",
    "    print(f\"   Rows: {df.shape[0]:,}\")\n",
    "    print(f\"   Columns: {df.shape[1]}\")\n",
    "    print(f\"   Total Cells: {df.shape[0] * df.shape[1]:,}\")\n",
    "    \n",
    "    print(\"\\nðŸ”¹ Memory Usage:\")\n",
    "    memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"   {memory:.2f} MB\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ‘€ STEP 3: PREVIEW DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nðŸ”¹ First 5 Rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\nðŸ”¹ Last 5 Rows:\")\n",
    "    print(df.tail())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ” STEP 4: MISSING VALUES ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Column': missing.index,\n",
    "        'Missing_Count': missing.values,\n",
    "        'Percentage': missing_pct.values\n",
    "    })\n",
    "    missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "    \n",
    "    if len(missing_df) > 0:\n",
    "        print(\"\\nColumns with Missing Values:\")\n",
    "        print(missing_df.to_string(index=False))\n",
    "        print(f\"\\nTotal missing values: {missing.sum():,}\")\n",
    "    else:\n",
    "        print(\"\\nNo missing values found!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 5: DUPLICATE ROWS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nDuplicate rows: {duplicates:,} ({duplicates/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    if duplicates > 0:\n",
    "        print(\"Consider removing duplicates during data cleaning\")\n",
    "    else:\n",
    "        print(\"No duplicates found!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 6: STATISTICAL SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nNumeric Columns:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    print(\"\\nCategorical Columns:\")\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        for col in categorical_cols:\n",
    "            print(f\"\\n   {col}:\")\n",
    "            print(f\"      Unique values: {df[col].nunique()}\")\n",
    "            print(f\"      Most common: {df[col].mode()[0] if len(df[col].mode()) > 0 else 'N/A'}\")\n",
    "            if df[col].nunique() <= 10:\n",
    "                print(f\"      Value counts:\")\n",
    "                print(df[col].value_counts().head(10).to_string())\n",
    "    else:\n",
    "        print(\"   No categorical columns found\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 7: DATA TYPES BREAKDOWN\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nðŸ”¹ Column Type Distribution:\")\n",
    "    dtype_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"   {dtype}: {count} columns\")\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "    \n",
    "    print(f\"\\n   Numeric columns ({len(numeric_cols)}): {numeric_cols}\")\n",
    "    print(f\"   Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "    print(f\"   Datetime columns ({len(datetime_cols)}): {datetime_cols}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 8: UNIQUE VALUES PER COLUMN\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    unique_df = pd.DataFrame({\n",
    "        'Column': df.columns,\n",
    "        'Unique_Values': [df[col].nunique() for col in df.columns],\n",
    "        'Unique_Percentage': [df[col].nunique()/len(df)*100 for col in df.columns]\n",
    "    }).sort_values('Unique_Values', ascending=False)\n",
    "    \n",
    "    print(unique_df.to_string(index=False))\n",
    "    \n",
    "    print(\"STEP 9: OUTLIER DETECTION\")\n",
    "    \n",
    "    print(\"\\nOutliers (IQR Method):\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    outlier_summary = []\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        outlier_count = len(outliers)\n",
    "        outlier_pct = (outlier_count / len(df)) * 100\n",
    "        \n",
    "        if outlier_count > 0:\n",
    "            outlier_summary.append({\n",
    "                'Column': col,\n",
    "                'Outliers': outlier_count,\n",
    "                'Percentage': f\"{outlier_pct:.2f}%\",\n",
    "                'Lower_Bound': f\"{lower_bound:.2f}\",\n",
    "                'Upper_Bound': f\"{upper_bound:.2f}\"\n",
    "            })\n",
    "    \n",
    "    if outlier_summary:\n",
    "        outlier_df = pd.DataFrame(outlier_summary)\n",
    "        print(outlier_df.to_string(index=False))\n",
    "    else:\n",
    "        print(\"   âœ… No outliers detected!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ”— STEP 10: CORRELATION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 1:\n",
    "        print(\"\\nðŸ”¹ Correlation Matrix:\")\n",
    "        corr_matrix = df[numeric_cols].corr()\n",
    "        print(corr_matrix)\n",
    "        \n",
    "        print(\"\\nðŸ”¹ High Correlations (|r| > 0.7):\")\n",
    "        high_corr = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                if abs(corr_matrix.iloc[i, j]) > 0.7:\n",
    "                    high_corr.append({\n",
    "                        'Feature_1': corr_matrix.columns[i],\n",
    "                        'Feature_2': corr_matrix.columns[j],\n",
    "                        'Correlation': f\"{corr_matrix.iloc[i, j]:.3f}\"\n",
    "                    })\n",
    "        \n",
    "        if high_corr:\n",
    "            high_corr_df = pd.DataFrame(high_corr)\n",
    "            print(high_corr_df.to_string(index=False))\n",
    "        else:\n",
    "            print(\"   No high correlations found\")\n",
    "    else:\n",
    "        print(\" Not enough numeric columns for correlation analysis\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 11: GENERATING VISUALIZATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if len(numeric_cols) > 0:\n",
    "        n_cols = min(len(numeric_cols), 4)\n",
    "        fig, axes = plt.subplots(n_cols, 1, figsize=(12, 4*n_cols))\n",
    "        if n_cols == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, col in enumerate(numeric_cols[:4]):\n",
    "            df[col].hist(bins=30, ax=axes[i], edgecolor='black', alpha=0.7)\n",
    "            axes[i].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "            axes[i].set_xlabel(col)\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"Distribution plots generated\")\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    if len(numeric_cols) > 1:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm', \n",
    "                    center=0, fmt='.2f', square=True, linewidths=1)\n",
    "        plt.title('Correlation Heatmap', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"Correlation heatmap generated\")\n",
    "    \n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        missing_data = df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending=False)\n",
    "        missing_data.plot(kind='bar', color='coral', edgecolor='black')\n",
    "        plt.title('Missing Values by Column', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Columns')\n",
    "        plt.ylabel('Missing Count')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"Missing values chart generated\")\n",
    "    \n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 12: EDA SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\"\"\n",
    "EDA COMPLETED SUCCESSFULLY!\n",
    "\n",
    "Key Findings:\n",
    "   â€¢ Total Rows: {df.shape[0]:,}\n",
    "   â€¢ Total Columns: {df.shape[1]}\n",
    "   â€¢ Missing Values: {df.isnull().sum().sum():,}\n",
    "   â€¢ Duplicate Rows: {duplicates:,}\n",
    "   â€¢ Numeric Columns: {len(df.select_dtypes(include=[np.number]).columns)}\n",
    "   â€¢ Categorical Columns: {len(df.select_dtypes(include=['object']).columns)}\n",
    "   â€¢ Memory Usage: {memory:.2f} MB\n",
    "\n",
    "Next Steps:\n",
    "   1. Handle missing values\n",
    "   2. Remove duplicates if necessary\n",
    "   3. Handle outliers\n",
    "   4. Proceed with data cleaning and feature engineering\n",
    "    \"\"\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = '/kaggle/input/retail/retail_data.csv'\n",
    "    \n",
    "    df = perform_eda(file_path)\n",
    "    \n",
    "    if df is not None:\n",
    "        print(\"\\nâœ… DataFrame is ready for further analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Final pipeline**************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************Pipeline dividing into 5 tables**********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T12:26:11.922839Z",
     "iopub.status.busy": "2025-11-15T12:26:11.921937Z",
     "iopub.status.idle": "2025-11-15T12:26:39.796363Z",
     "shell.execute_reply": "2025-11-15T12:26:39.795415Z",
     "shell.execute_reply.started": "2025-11-15T12:26:11.922809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def parse_mixed_date(series: pd.Series) -> pd.Series:\n",
    "    s = series.astype(str)\n",
    "    res = pd.Series(pd.NaT, index=s.index, dtype='datetime64[ns]')\n",
    "    mask_slash = s.str.contains(\"/\")\n",
    "    mask_dash = s.str.contains(\"-\")\n",
    "\n",
    "    res.loc[mask_slash] = pd.to_datetime(s.loc[mask_slash],\n",
    "        format=\"%m/%d/%Y\", errors=\"coerce\")\n",
    "\n",
    "    res.loc[mask_dash] = pd.to_datetime(s.loc[mask_dash],\n",
    "        format=\"%m-%d-%y\", errors=\"coerce\")\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def safe_int_str(series: pd.Series) -> pd.Series:\n",
    "    return series.astype(\"Int64\").astype(str)\n",
    "\n",
    "\n",
    "CSV_PATH = \"/kaggle/input/good-dataset/good_data.csv\"\n",
    "DB_URL = \"sqlite:///retail.db\"\n",
    "\n",
    "print(\"ðŸ”¹ Loading CSV...\")\n",
    "raw = pd.read_csv(CSV_PATH)\n",
    "\n",
    "raw[\"tx_date\"] = parse_mixed_date(raw[\"Date\"])\n",
    "raw[\"customer_id\"] = safe_int_str(raw[\"Customer_ID\"])\n",
    "raw[\"transaction_id\"] = safe_int_str(raw[\"Transaction_ID\"])\n",
    "\n",
    "\n",
    "def build_customer_master(df):\n",
    "    g = df.dropna(subset=[\"customer_id\"]).groupby(\"customer_id\")\n",
    "\n",
    "    cm = g.agg({\n",
    "        \"Name\":\"first\",\"Email\":\"first\",\"Phone\":\"first\",\n",
    "        \"Address\":\"first\",\"City\":\"first\",\"State\":\"first\",\n",
    "        \"Zipcode\":\"first\",\"Country\":\"first\",\"Age\":\"first\",\n",
    "        \"Gender\":\"first\",\"Income\":\"first\",\"Customer_Segment\":\"first\",\n",
    "        \"tx_date\":[\"min\",\"max\"]\n",
    "    }).reset_index()\n",
    "\n",
    "    cm.columns = [\n",
    "        \"customer_id\",\"name\",\"email\",\"phone\",\"address\",\"city\",\"state\",\"zipcode\",\n",
    "        \"country\",\"age\",\"gender\",\"income\",\"customer_segment\",\n",
    "        \"customer_since\",\"last_purchase_date\"\n",
    "    ]\n",
    "\n",
    "    cm[\"is_loyalty_member\"] = True\n",
    "    cm[\"total_loyalty_points\"] = 0\n",
    "    cm[\"bonus_points\"] = 0\n",
    "    cm[\"last_points_update\"] = pd.NaT\n",
    "\n",
    "    cm[\"zipcode\"] = cm[\"zipcode\"].astype(\"Int64\").astype(str)\n",
    "    cm[\"age\"] = cm[\"age\"].astype(\"Int64\")\n",
    "    cm[\"income\"] = pd.to_numeric(cm[\"income\"], errors=\"coerce\")\n",
    "\n",
    "    # CLEAN duplicated keys\n",
    "    cm = cm.drop_duplicates(subset=[\"customer_id\"])\n",
    "\n",
    "    return cm\n",
    "\n",
    "\n",
    "def build_product_master(df):\n",
    "    pm = df[[\"Product_Category\",\"Product_Brand\",\"Product_Type\",\"products\"]].drop_duplicates()\n",
    "    pm = pm.rename(columns={\n",
    "        \"Product_Category\":\"product_category\",\n",
    "        \"Product_Brand\":\"product_brand\",\n",
    "        \"Product_Type\":\"product_type\",\n",
    "        \"products\":\"product_full_name\"\n",
    "    })\n",
    "\n",
    "    pm = pm.reset_index(drop=True)\n",
    "    pm[\"product_key\"] = pm.index.to_series().add(1).map(lambda x: f\"P{x:06d}\")\n",
    "    pm[\"is_active\"] = True\n",
    "\n",
    "    return pm[[\n",
    "        \"product_key\",\"product_category\",\"product_brand\",\"product_type\",\n",
    "        \"product_full_name\",\"is_active\"\n",
    "    ]]\n",
    "\n",
    "\n",
    "def build_sales_transactions(df):\n",
    "    st = df.copy()\n",
    "    st[\"date\"] = st[\"tx_date\"]\n",
    "    st[\"year\"] = st[\"date\"].dt.year\n",
    "    st[\"month\"] = st[\"date\"].dt.month\n",
    "\n",
    "    month_map = {\n",
    "        \"January\":1,\"February\":2,\"March\":3,\"April\":4,\"May\":5,\"June\":6,\n",
    "        \"July\":7,\"August\":8,\"September\":9,\"October\":10,\"November\":11,\"December\":12\n",
    "    }\n",
    "    nat_mask = st[\"date\"].isna()\n",
    "    st.loc[nat_mask,\"year\"] = st.loc[nat_mask,\"Year\"].astype(\"Int64\")\n",
    "    st.loc[nat_mask,\"month\"] = st.loc[nat_mask,\"Month\"].map(month_map)\n",
    "\n",
    "    st[\"ingestion_timestamp\"] = datetime.utcnow()\n",
    "    st[\"data_quality_flag\"] = \"PASS\"\n",
    "    st[\"reject_reason\"] = pd.NA\n",
    "\n",
    "    st_df = st.rename(columns={\n",
    "        \"Time\":\"time\",\n",
    "        \"Total_Purchases\":\"total_purchases\",\n",
    "        \"Amount\":\"amount\",\n",
    "        \"Total_Amount\":\"total_amount\",\n",
    "        \"Product_Category\":\"product_category\",\n",
    "        \"Product_Brand\":\"product_brand\",\n",
    "        \"Product_Type\":\"product_type\",\n",
    "        \"Shipping_Method\":\"shipping_method\",\n",
    "        \"Payment_Method\":\"payment_method\",\n",
    "        \"Order_Status\":\"order_status\",\n",
    "        \"Ratings\":\"ratings\",\n",
    "        \"Feedback\":\"feedback\"\n",
    "    })\n",
    "\n",
    "    st_df = st_df[[\n",
    "        \"transaction_id\",\"customer_id\",\"date\",\"year\",\"month\",\"time\",\n",
    "        \"total_purchases\",\"amount\",\"total_amount\",\"product_category\",\n",
    "        \"product_brand\",\"product_type\",\"shipping_method\",\"payment_method\",\n",
    "        \"order_status\",\"ratings\",\"feedback\",\"ingestion_timestamp\",\n",
    "        \"data_quality_flag\",\"reject_reason\"\n",
    "    ]]\n",
    "\n",
    "    # CLEAN null + duplicate transaction IDs\n",
    "    st_df = st_df.dropna(subset=[\"transaction_id\"])\n",
    "    st_df = st_df[st_df[\"transaction_id\"] != \"<NA>\"]\n",
    "    st_df = st_df.drop_duplicates(subset=[\"transaction_id\"])\n",
    "\n",
    "    return st_df\n",
    "\n",
    "\n",
    "def build_customer_analytics(st_df):\n",
    "    snapshot_date = st_df[\"date\"].max()\n",
    "    g = st_df.groupby(\"customer_id\")\n",
    "\n",
    "    base = g.agg({\n",
    "        \"date\":\"max\",\"transaction_id\":\"nunique\",\"total_amount\":\"sum\",\n",
    "        \"product_type\":\"nunique\",\"ratings\":\"mean\"\n",
    "    }).reset_index()\n",
    "\n",
    "    base = base.rename(columns={\n",
    "        \"date\":\"last_purchase_date\",\"transaction_id\":\"frequency\",\n",
    "        \"total_amount\":\"monetary\",\"product_type\":\"product_diversity\",\n",
    "        \"ratings\":\"avg_rating\"\n",
    "    })\n",
    "\n",
    "    base[\"recency\"] = (snapshot_date - base[\"last_purchase_date\"]).dt.days\n",
    "    base[\"avg_rating\"] = base[\"avg_rating\"].fillna(0)\n",
    "    base[\"monetary\"] = base[\"monetary\"].fillna(0)\n",
    "    base[\"recency\"] = base[\"recency\"].fillna(base[\"recency\"].max())\n",
    "\n",
    "    base[\"rfm_score\"] = 1\n",
    "    base[\"segment\"] = \"Medium\"\n",
    "    base[\"clv_score\"] = (\n",
    "        base[\"monetary\"] * base[\"frequency\"] / (base[\"recency\"] + 1)\n",
    "    )\n",
    "\n",
    "    base[\"snapshot_date\"] = snapshot_date\n",
    "\n",
    "    return base[[\n",
    "        \"customer_id\",\"recency\",\"frequency\",\"monetary\",\n",
    "        \"rfm_score\",\"segment\",\"product_diversity\",\"avg_rating\",\n",
    "        \"clv_score\",\"snapshot_date\"\n",
    "    ]]\n",
    "\n",
    "\n",
    "def build_loyalty_transactions(st_df):\n",
    "    lt = st_df[[\n",
    "        \"transaction_id\",\"customer_id\",\"date\",\"total_amount\"\n",
    "    ]].copy()\n",
    "\n",
    "    lt[\"points_earned\"] = lt[\"total_amount\"].fillna(0).floordiv(10).astype(int)\n",
    "    lt[\"points_redeemed\"] = 0\n",
    "    lt[\"bonus_points\"] = 0\n",
    "\n",
    "    lt = lt.sort_values([\"customer_id\",\"date\",\"transaction_id\"])\n",
    "    lt[\"balance_after\"] = lt.groupby(\"customer_id\")[\"points_earned\"].cumsum()\n",
    "\n",
    "    lt[\"event_date\"] = lt[\"date\"]\n",
    "    lt[\"event_type\"] = \"EARN\"\n",
    "\n",
    "    lt = lt.reset_index(drop=True)\n",
    "    lt[\"loyalty_txn_id\"] = lt.index.to_series().add(1).map(lambda x: f\"L{x:07d}\")\n",
    "\n",
    "    return lt[[\n",
    "        \"loyalty_txn_id\",\"customer_id\",\"transaction_id\",\"points_earned\",\n",
    "        \"points_redeemed\",\"bonus_points\",\"balance_after\",\"event_date\",\n",
    "        \"event_type\"\n",
    "    ]]\n",
    "\n",
    "print(\"ðŸ”¹ Building normalized tables...\")\n",
    "\n",
    "customer_master_df = build_customer_master(raw)\n",
    "product_master_df = build_product_master(raw)\n",
    "sales_transactions_df = build_sales_transactions(raw)\n",
    "customer_analytics_df = build_customer_analytics(sales_transactions_df)\n",
    "loyalty_transactions_df = build_loyalty_transactions(sales_transactions_df)\n",
    "\n",
    "\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "DDL = [\n",
    "    \"DROP TABLE IF EXISTS loyalty_transactions;\",\n",
    "    \"DROP TABLE IF EXISTS customer_analytics;\",\n",
    "    \"DROP TABLE IF EXISTS sales_transactions;\",\n",
    "    \"DROP TABLE IF EXISTS product_master;\",\n",
    "    \"DROP TABLE IF EXISTS customer_master;\",\n",
    "\n",
    "    \"\"\"\n",
    "    CREATE TABLE customer_master (\n",
    "        customer_id VARCHAR PRIMARY KEY,\n",
    "        name VARCHAR,\n",
    "        email VARCHAR,\n",
    "        phone VARCHAR,\n",
    "        address VARCHAR,\n",
    "        city VARCHAR,\n",
    "        state VARCHAR,\n",
    "        zipcode VARCHAR,\n",
    "        country VARCHAR,\n",
    "        age INT,\n",
    "        gender VARCHAR,\n",
    "        income DECIMAL,\n",
    "        customer_segment VARCHAR,\n",
    "        is_loyalty_member BOOLEAN,\n",
    "        customer_since DATE,\n",
    "        total_loyalty_points INT,\n",
    "        bonus_points INT,\n",
    "        last_points_update DATE,\n",
    "        last_purchase_date DATE\n",
    "    );\n",
    "    \"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "    CREATE TABLE product_master (\n",
    "        product_key VARCHAR PRIMARY KEY,\n",
    "        product_category VARCHAR,\n",
    "        product_brand VARCHAR,\n",
    "        product_type VARCHAR,\n",
    "        product_full_name VARCHAR,\n",
    "        is_active BOOLEAN\n",
    "    );\n",
    "    \"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "    CREATE TABLE sales_transactions (\n",
    "        transaction_id VARCHAR PRIMARY KEY,\n",
    "        customer_id VARCHAR,\n",
    "        date DATE,\n",
    "        year INT,\n",
    "        month INT,\n",
    "        time VARCHAR,\n",
    "        total_purchases INT,\n",
    "        amount DECIMAL,\n",
    "        total_amount DECIMAL,\n",
    "        product_category VARCHAR,\n",
    "        product_brand VARCHAR,\n",
    "        product_type VARCHAR,\n",
    "        shipping_method VARCHAR,\n",
    "        payment_method VARCHAR,\n",
    "        order_status VARCHAR,\n",
    "        ratings INT,\n",
    "        feedback TEXT,\n",
    "        ingestion_timestamp TIMESTAMP,\n",
    "        data_quality_flag VARCHAR,\n",
    "        reject_reason TEXT,\n",
    "        FOREIGN KEY(customer_id) REFERENCES\tcustomer_master(customer_id)\n",
    "    );\n",
    "    \"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "    CREATE TABLE customer_analytics (\n",
    "        customer_id VARCHAR PRIMARY KEY,\n",
    "        recency INT,\n",
    "        frequency INT,\n",
    "        monetary DECIMAL,\n",
    "        rfm_score INT,\n",
    "        segment VARCHAR,\n",
    "        product_diversity INT,\n",
    "        avg_rating DECIMAL,\n",
    "        clv_score DECIMAL,\n",
    "        snapshot_date DATE,\n",
    "        FOREIGN KEY(customer_id) REFERENCES customer_master(customer_id)\n",
    "    );\n",
    "    \"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "    CREATE TABLE loyalty_transactions (\n",
    "        loyalty_txn_id VARCHAR PRIMARY KEY,\n",
    "        customer_id VARCHAR,\n",
    "        transaction_id VARCHAR,\n",
    "        points_earned INT,\n",
    "        points_redeemed INT,\n",
    "        bonus_points INT,\n",
    "        balance_after INT,\n",
    "        event_date DATE,\n",
    "        event_type VARCHAR,\n",
    "        FOREIGN KEY(customer_id) REFERENCES customer_master(customer_id),\n",
    "        FOREIGN KEY(transaction_id) REFERENCES sales_transactions(transaction_id)\n",
    "    );\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ”¹ Creating database schema...\")\n",
    "with engine.begin() as conn:\n",
    "    conn.exec_driver_sql(\"PRAGMA foreign_keys = OFF;\")  \n",
    "    for stmt in DDL:\n",
    "        conn.exec_driver_sql(stmt)\n",
    "    conn.exec_driver_sql(\"PRAGMA foreign_keys = ON;\")    \n",
    "\n",
    "\n",
    "print(\"ðŸ”¹ Loading data into DB...\")\n",
    "\n",
    "customer_master_df.to_sql(\"customer_master\", engine, if_exists=\"append\", index=False)\n",
    "product_master_df.to_sql(\"product_master\", engine, if_exists=\"append\", index=False)\n",
    "sales_transactions_df.to_sql(\"sales_transactions\", engine, if_exists=\"append\", index=False)\n",
    "customer_analytics_df.to_sql(\"customer_analytics\", engine, if_exists=\"append\", index=False)\n",
    "loyalty_transactions_df.to_sql(\"loyalty_transactions\", engine, if_exists=\"append\", index=False)\n",
    "\n",
    "print(\"âœ… ETL pipeline finished. retail.db created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sqlite database connections**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T12:27:03.678968Z",
     "iopub.status.busy": "2025-11-15T12:27:03.678631Z",
     "iopub.status.idle": "2025-11-15T12:27:03.683969Z",
     "shell.execute_reply": "2025-11-15T12:27:03.683030Z",
     "shell.execute_reply.started": "2025-11-15T12:27:03.678942Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect(\"retail.db\")\n",
    "cursor = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T12:27:09.930194Z",
     "iopub.status.busy": "2025-11-15T12:27:09.929847Z",
     "iopub.status.idle": "2025-11-15T12:27:09.937281Z",
     "shell.execute_reply": "2025-11-15T12:27:09.936418Z",
     "shell.execute_reply.started": "2025-11-15T12:27:09.930168Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "cursor.fetchall()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer_master table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T12:27:13.219045Z",
     "iopub.status.busy": "2025-11-15T12:27:13.218744Z",
     "iopub.status.idle": "2025-11-15T12:27:13.225233Z",
     "shell.execute_reply": "2025-11-15T12:27:13.224153Z",
     "shell.execute_reply.started": "2025-11-15T12:27:13.219022Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"PRAGMA table_info(customer_master);\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "for r in rows:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T12:27:16.963019Z",
     "iopub.status.busy": "2025-11-15T12:27:16.962350Z",
     "iopub.status.idle": "2025-11-15T12:27:16.992627Z",
     "shell.execute_reply": "2025-11-15T12:27:16.991889Z",
     "shell.execute_reply.started": "2025-11-15T12:27:16.962990Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM sales_transactions LIMIT 10;\", conn)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "product_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T12:27:22.742553Z",
     "iopub.status.busy": "2025-11-15T12:27:22.742220Z",
     "iopub.status.idle": "2025-11-15T12:27:22.748462Z",
     "shell.execute_reply": "2025-11-15T12:27:22.747528Z",
     "shell.execute_reply.started": "2025-11-15T12:27:22.742530Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"PRAGMA table_info(product_master);\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "for r in rows:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T12:27:26.130027Z",
     "iopub.status.busy": "2025-11-15T12:27:26.129714Z",
     "iopub.status.idle": "2025-11-15T12:27:26.142767Z",
     "shell.execute_reply": "2025-11-15T12:27:26.141665Z",
     "shell.execute_reply.started": "2025-11-15T12:27:26.130002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM product_master LIMIT 10 ;\", conn)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sales_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T12:27:30.208595Z",
     "iopub.status.busy": "2025-11-15T12:27:30.208236Z",
     "iopub.status.idle": "2025-11-15T12:27:30.214321Z",
     "shell.execute_reply": "2025-11-15T12:27:30.213588Z",
     "shell.execute_reply.started": "2025-11-15T12:27:30.208566Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"PRAGMA table_info(sales_transactions);\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "for r in rows:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T12:27:58.814043Z",
     "iopub.status.busy": "2025-11-15T12:27:58.813741Z",
     "iopub.status.idle": "2025-11-15T12:27:58.834973Z",
     "shell.execute_reply": "2025-11-15T12:27:58.834265Z",
     "shell.execute_reply.started": "2025-11-15T12:27:58.814019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM sales_transactions LIMIT 10 ;\", conn)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "customer_analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T12:30:51.521447Z",
     "iopub.status.busy": "2025-11-15T12:30:51.520372Z",
     "iopub.status.idle": "2025-11-15T12:30:51.527020Z",
     "shell.execute_reply": "2025-11-15T12:30:51.526121Z",
     "shell.execute_reply.started": "2025-11-15T12:30:51.521408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"PRAGMA table_info(customer_analytics);\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "for r in rows:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T12:31:32.689617Z",
     "iopub.status.busy": "2025-11-15T12:31:32.689339Z",
     "iopub.status.idle": "2025-11-15T12:31:32.704916Z",
     "shell.execute_reply": "2025-11-15T12:31:32.704008Z",
     "shell.execute_reply.started": "2025-11-15T12:31:32.689596Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM customer_analytics LIMIT 10 ;\", conn)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loyalty_transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T12:36:57.392947Z",
     "iopub.status.busy": "2025-11-15T12:36:57.392645Z",
     "iopub.status.idle": "2025-11-15T12:36:57.398568Z",
     "shell.execute_reply": "2025-11-15T12:36:57.397835Z",
     "shell.execute_reply.started": "2025-11-15T12:36:57.392927Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"PRAGMA table_info(loyalty_transactions);\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "for r in rows:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T12:37:00.051811Z",
     "iopub.status.busy": "2025-11-15T12:37:00.050981Z",
     "iopub.status.idle": "2025-11-15T12:37:00.064358Z",
     "shell.execute_reply": "2025-11-15T12:37:00.063531Z",
     "shell.execute_reply.started": "2025-11-15T12:37:00.051782Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM loyalty_transactions LIMIT 10 ;\", conn)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8743459,
     "sourceId": 13741560,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8743864,
     "sourceId": 13742091,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
